{
 "metadata": {
  "name": "",
  "signature": "sha256:3802960395e64282d97ef24bd8391fd94a43c0869c411fbf96414d64a67152d9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys, os\n",
      "import pandas as pd\n",
      "import cPickle as pickle\n",
      "import re\n",
      "import numpy as np\n",
      "import nltk\n",
      "\n",
      "%matplotlib \n",
      "\n",
      "os.chdir('C:/Users/Asaf/Documents/GitHub/TwitterIdentity/src/py')\n",
      "\n",
      "\n",
      "\n",
      "survey=pd.read_pickle('DataFrames/SurveySentiment')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using matplotlib backend: Qt4Agg\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import string \n",
      "\n",
      "def read_liwc(filename):\n",
      "    liwc_data = open(filename, 'r')\n",
      "    \n",
      "    mode = 0\n",
      "    cat = {}\n",
      "    dic = {}\n",
      "\n",
      "    for line in liwc_data:\n",
      "        line = line.strip('\\r\\n')\n",
      "        if line == '%':\n",
      "            mode += 1\n",
      "            continue\n",
      "        elif mode == 1: # cat\n",
      "            chunks = line.split('\\t')\n",
      "            cat[chunks[0]] = chunks[1]\n",
      "        elif mode == 2: # dic\n",
      "            chunks = line.split('\\t')\n",
      "            word = chunks.pop(0)\n",
      "            dic[word] = chunks\n",
      "    return (cat, dic)\n",
      "\n",
      "\n",
      "def get_wordsets(dic, looks):\n",
      "    rs = {}\n",
      "    for key in looks:\n",
      "        rs[key] = {}\n",
      "    for word in dic:\n",
      "        for (key, value) in looks.items():# for each filed, I need to look at\n",
      "            for cat in dic[word]:\n",
      "                if cat in value: # if find right cat\n",
      "                    rs[key][word] = dic[word]\n",
      "                    continue\n",
      "    return rs\n",
      "                    \n",
      "def score(txt, looks, looks_ws, show=False):\n",
      "    txt = txt.lower()\n",
      "    words = txt.split(' ')\n",
      "    words_count = len(words)\n",
      "    rs = {}\n",
      "    for key in looks:\n",
      "        rs[key] = 0\n",
      "    for word in words:\n",
      "        if len(word) == 0 or word[0] == '@': # empty word, or this word is name\n",
      "            continue \n",
      "        word = word.translate(None, string.punctuation)\n",
      "        for key in looks:\n",
      "            wordsets = looks_ws[key] # get complete set for individual target\n",
      "            for wordset in wordsets:\n",
      "                if matches(wordset, word): # check for value, can improve accuracy here\n",
      "                    if show: # print corps\n",
      "                        print wordset, ' = ', key \n",
      "                    rs[key] = rs[key]+1\n",
      "    return (words_count, rs)\n",
      "\n",
      "\n",
      "#### needs to update here for complex matches\n",
      "def matches(liwc_word, txt):\n",
      "    if liwc_word[-1] == '*':\n",
      "        return txt.startswith(liwc_word[:-1])\n",
      "    else:\n",
      "        return txt == liwc_word"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent_statuses=nltk.word_tokenize(sent_statuses.lower())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z=nltk.FreqDist(sent_statuses)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "survey['tw_liwc']=None #Initialize column for Liwc scores.\n",
      "survey['tw_tot_words']=None \n",
      "#cat gives us the dictionaries of words that we are looking for and their associated categories, cat[1], as well as the Liwc categories\n",
      "#\n",
      "cat=read_liwc('liwc_file.txt')\n",
      "\n",
      "tw_ids=os.listdir('B:/Twitter_Part')\n",
      "path='B:/Twitter_Part/'\n",
      "error_log=[]\n",
      "\n",
      " # take every tweet's sentiment and combine them all. \n",
      "\n",
      "\n",
      "for j in tw_ids:\n",
      "\n",
      "    sent_statuses=''# This is just the collection of all statuses. \n",
      "    filename=path+str(j)+'/tweets'\n",
      "    \n",
      "    try:\n",
      "\n",
      "        z=pickle.load(open(filename))\n",
      "        sent_path=path+str(j)+'/LIWC'\n",
      "        for status in z:\n",
      "            if 'text' in status.keys():\n",
      "                if status['text'] != 'None':\n",
      "                    message=status['text'].encode('utf-8')\n",
      "\n",
      "\n",
      "                    #Here is where I need to reference some cool function that does the appropriate stuff with LIWC\n",
      "\n",
      "                    # Features- Takes in a text string, tokenizes it using nltk, also, make it lower case. \n",
      "                    # Two ways to to do it. Just, agregate all of a person's info- yeah, just agregate for now \n",
      "\n",
      "                    sent_statuses+= ' '+ message\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        sent_statuses=nltk.word_tokenize(sent_statuses.lower())\n",
      "        freq=nltk.FreqDist(sent_statuses)\n",
      "        words=freq.keys()\n",
      "        score={}\n",
      "        score2={}\n",
      "        for entry in cat[0].keys():\n",
      "            score[entry]=0\n",
      "        #Initialize a dictionary to keep track of the score for each category- at the end if a category is not represented\n",
      "        #I can add it as a key with frequency 0. Also, for now I can add up all all the words used in Tweets- but it won't be entirely accurate\n",
      "        #because of the format and the way things were tokenized. I can do relative scores for distributions. \n",
      "\n",
      "        for i in cat[1].keys():# each word in the Liwc dictionary- check to see if it is in in the frequency distribution\n",
      "            if i[-1]=='*':\n",
      "                candidates=[j for j in words if j.startswith(i[0:-1])]\n",
      "                if len(candidates) > 0: #If at least one of these stems is present\n",
      "                    tot_stems=0 # initialize a variable that will represent the total number of occurences with a given stem\n",
      "                    for k in candidates:\n",
      "                        tot_stems+=freq[k]# Add the frequence for each variation\n",
      "                    to_tally=cat[1][i]\n",
      "                    for categ in to_tally:\n",
      "                        score[categ]=tot_stems\n",
      "\n",
      "            else:\n",
      "                if i in words:\n",
      "                    to_tally=cat[1][i]\n",
      "                    for categ in to_tally:\n",
      "                        score[categ]=freq[i]\n",
      "\n",
      "\n",
      "\n",
      "        for i in score.keys():\n",
      "            if i in cat[0].keys():\n",
      "                name=cat[0][i]\n",
      "                score2[name]=int(score[i])\n",
      "\n",
      "        all_words=len(sent_statuses)\n",
      "\n",
      "        survey.tw_liwc[survey.Twitid==j]=[score2]\n",
      "        survey.tw_tot_words[survey.Twitid==j]=all_words\n",
      "\n",
      "                \n",
      "          \n",
      "                \n",
      "                    \n",
      "                \n",
      "       # else:\n",
      "            \n",
      "            \n",
      "            \n",
      "        \n",
      "   \n",
      "        \n",
      "    \n",
      "    except Exception, e:\n",
      "        error_log.append(j)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z=[score2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "[{'achieve': 2,\n",
        "  'adverb': 3,\n",
        "  'affect': 1,\n",
        "  'anger': 2,\n",
        "  'anx': 1,\n",
        "  'article': 547,\n",
        "  'assent': 1,\n",
        "  'auxverb': 52,\n",
        "  'bio': 7,\n",
        "  'body': 7,\n",
        "  'cause': 11,\n",
        "  'certain': 1,\n",
        "  'cogmech': 11,\n",
        "  'conj': 117,\n",
        "  'death': 3,\n",
        "  'discrep': 1,\n",
        "  'excl': 117,\n",
        "  'family': 2,\n",
        "  'feel': 23,\n",
        "  'filler': 1,\n",
        "  'friend': 1,\n",
        "  'funct': 3,\n",
        "  'future': 68,\n",
        "  'health': 1,\n",
        "  'hear': 2,\n",
        "  'home': 1,\n",
        "  'humans': 1,\n",
        "  'i': 1502,\n",
        "  'incl': 106,\n",
        "  'ingest': 1,\n",
        "  'inhib': 1,\n",
        "  'insight': 11,\n",
        "  'ipron': 94,\n",
        "  'leisure': 1,\n",
        "  'money': 27,\n",
        "  'motion': 1,\n",
        "  'negate': 3,\n",
        "  'negemo': 1,\n",
        "  'nonfl': 31,\n",
        "  'number': 7,\n",
        "  'past': 15,\n",
        "  'percept': 2,\n",
        "  'posemo': 6,\n",
        "  'ppron': 705,\n",
        "  'preps': 119,\n",
        "  'present': 1,\n",
        "  'pronoun': 94,\n",
        "  'quant': 6,\n",
        "  'relativ': 1,\n",
        "  'relig': 1,\n",
        "  'sad': 1,\n",
        "  'see': 1,\n",
        "  'sexual': 1,\n",
        "  'shehe': 58,\n",
        "  'social': 1,\n",
        "  'space': 1,\n",
        "  'swear': 4,\n",
        "  'tentat': 9,\n",
        "  'they': 22,\n",
        "  'time': 1,\n",
        "  'verb': 1,\n",
        "  'we': 106,\n",
        "  'work': 2,\n",
        "  'you': 705}]"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cat=read_liwc('liwc_file.txt')\n",
      "\n",
      "\n",
      "freq=nltk.FreqDist(sent_statuses)\n",
      "words=freq.keys()\n",
      "score={}\n",
      "for entry in cat[0].keys():\n",
      "    score[entry]=0\n",
      "#Initialize a dictionary to keep track of the score for each category- at the end if a category is not represented\n",
      "#I can add it as a key with frequency 0. Also, for now I can add up all all the words used in Tweets- but it won't be entirely accurate\n",
      "#because of the format and the way things were tokenized. I can do relative scores for distributions. \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "candidates=[j for j in words if j.startswith('party')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "freq['party']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 77,
       "text": [
        "2"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in cat[1].keys():# each word in the Liwc dictionary- check to see if it is in in the frequency distribution\n",
      "    if i[-1]=='*':\n",
      "        candidates=[j for j in words if j.startswith(i[0:-1])]\n",
      "        if len(candidates) > 0: #If at least one of these stems is present\n",
      "            tot_stems=0 # initialize a variable that will represent the total number of occurences with a given stem\n",
      "            for k in candidates:\n",
      "                tot_stems+=freq[k]# Add the frequence for each variation\n",
      "            to_tally=cat[1][i]\n",
      "            for categ in to_tally:\n",
      "                score[categ]=tot_stems\n",
      "\n",
      "score2={}\n",
      "\n",
      "for i in score.keys():\n",
      "    name=cat[0][i]\n",
      "    score2[name]=score[i]\n",
      "                \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "score2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 82,
       "text": [
        "{'achieve': 6,\n",
        " 'adverb': 1,\n",
        " 'affect': 1,\n",
        " 'anger': 1,\n",
        " 'anx': 1,\n",
        " 'article': 0,\n",
        " 'assent': 2,\n",
        " 'auxverb': 0,\n",
        " 'bio': 1,\n",
        " 'body': 1,\n",
        " 'cause': 4,\n",
        " 'certain': 4,\n",
        " 'cogmech': 4,\n",
        " 'conj': 0,\n",
        " 'death': 1,\n",
        " 'discrep': 5,\n",
        " 'excl': 3,\n",
        " 'family': 1,\n",
        " 'feel': 1,\n",
        " 'filler': 0,\n",
        " 'friend': 12,\n",
        " 'funct': 1,\n",
        " 'future': 0,\n",
        " 'health': 1,\n",
        " 'hear': 1,\n",
        " 'home': 1,\n",
        " 'humans': 1,\n",
        " 'i': 0,\n",
        " 'incl': 0,\n",
        " 'ingest': 1,\n",
        " 'inhib': 1,\n",
        " 'insight': 4,\n",
        " 'ipron': 1,\n",
        " 'leisure': 2,\n",
        " 'money': 1,\n",
        " 'motion': 1,\n",
        " 'negate': 1,\n",
        " 'negemo': 1,\n",
        " 'nonfl': 0,\n",
        " 'number': 1,\n",
        " 'past': 0,\n",
        " 'percept': 1,\n",
        " 'posemo': 2,\n",
        " 'ppron': 6,\n",
        " 'preps': 1,\n",
        " 'present': 0,\n",
        " 'pronoun': 1,\n",
        " 'quant': 1,\n",
        " 'relativ': 1,\n",
        " 'relig': 1,\n",
        " 'sad': 1,\n",
        " 'see': 2,\n",
        " 'sexual': 3,\n",
        " 'shehe': 0,\n",
        " 'social': 1,\n",
        " 'space': 2,\n",
        " 'swear': 3,\n",
        " 'tentat': 1,\n",
        " 'they': 6,\n",
        " 'time': 1,\n",
        " 'verb': 0,\n",
        " 'we': 0,\n",
        " 'work': 5,\n",
        " 'you': 0}"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cat[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 80,
       "text": [
        "{'1': 'funct',\n",
        " '10': 'article',\n",
        " '11': 'verb',\n",
        " '12': 'auxverb',\n",
        " '121': 'social',\n",
        " '122': 'family',\n",
        " '123': 'friend',\n",
        " '124': 'humans',\n",
        " '125': 'affect',\n",
        " '126': 'posemo',\n",
        " '127': 'negemo',\n",
        " '128': 'anx',\n",
        " '129': 'anger',\n",
        " '13': 'past',\n",
        " '130': 'sad',\n",
        " '131': 'cogmech',\n",
        " '132': 'insight',\n",
        " '133': 'cause',\n",
        " '134': 'discrep',\n",
        " '135': 'tentat',\n",
        " '136': 'certain',\n",
        " '137': 'inhib',\n",
        " '138': 'incl',\n",
        " '139': 'excl',\n",
        " '14': 'present',\n",
        " '140': 'percept',\n",
        " '141': 'see',\n",
        " '142': 'hear',\n",
        " '143': 'feel',\n",
        " '146': 'bio',\n",
        " '147': 'body',\n",
        " '148': 'health',\n",
        " '149': 'sexual',\n",
        " '15': 'future',\n",
        " '150': 'ingest',\n",
        " '16': 'adverb',\n",
        " '17': 'preps',\n",
        " '18': 'conj',\n",
        " '19': 'negate',\n",
        " '2': 'pronoun',\n",
        " '20': 'quant',\n",
        " '21': 'number',\n",
        " '22': 'swear',\n",
        " '250': 'relativ',\n",
        " '251': 'motion',\n",
        " '252': 'space',\n",
        " '253': 'time',\n",
        " '3': 'ppron',\n",
        " '354': 'work',\n",
        " '355': 'achieve',\n",
        " '356': 'leisure',\n",
        " '357': 'home',\n",
        " '358': 'money',\n",
        " '359': 'relig',\n",
        " '360': 'death',\n",
        " '4': 'i',\n",
        " '462': 'assent',\n",
        " '463': 'nonfl',\n",
        " '464': 'filler',\n",
        " '5': 'we',\n",
        " '6': 'you',\n",
        " '7': 'shehe',\n",
        " '8': 'they',\n",
        " '9': 'ipron'}"
       ]
      }
     ],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "score\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 83,
       "text": [
        "{'1': 1,\n",
        " '10': 0,\n",
        " '11': 0,\n",
        " '12': 0,\n",
        " '121': 1,\n",
        " '122': 1,\n",
        " '123': 12,\n",
        " '124': 1,\n",
        " '125': 1,\n",
        " '126': 2,\n",
        " '127': 1,\n",
        " '128': 1,\n",
        " '129': 1,\n",
        " '13': 0,\n",
        " '130': 1,\n",
        " '131': 4,\n",
        " '132': 4,\n",
        " '133': 4,\n",
        " '134': 5,\n",
        " '135': 1,\n",
        " '136': 4,\n",
        " '137': 1,\n",
        " '138': 0,\n",
        " '139': 3,\n",
        " '14': 0,\n",
        " '140': 1,\n",
        " '141': 2,\n",
        " '142': 1,\n",
        " '143': 1,\n",
        " '146': 1,\n",
        " '147': 1,\n",
        " '148': 1,\n",
        " '149': 3,\n",
        " '15': 0,\n",
        " '150': 1,\n",
        " '16': 1,\n",
        " '17': 1,\n",
        " '18': 0,\n",
        " '19': 1,\n",
        " '2': 1,\n",
        " '20': 1,\n",
        " '21': 1,\n",
        " '22': 3,\n",
        " '250': 1,\n",
        " '251': 1,\n",
        " '252': 2,\n",
        " '253': 1,\n",
        " '3': 6,\n",
        " '354': 5,\n",
        " '355': 6,\n",
        " '356': 2,\n",
        " '357': 1,\n",
        " '358': 1,\n",
        " '359': 1,\n",
        " '360': 1,\n",
        " '4': 0,\n",
        " '462': 2,\n",
        " '463': 0,\n",
        " '464': 0,\n",
        " '5': 0,\n",
        " '6': 0,\n",
        " '7': 0,\n",
        " '8': 6,\n",
        " '9': 1}"
       ]
      }
     ],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent=nltk.word_tokenize(a)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "['i', 'am', 'so', 'cool']"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z=read_liwc('liwc_file.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(z[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "64"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "{'a': ['1', '10'],\n",
        " 'abandon*': ['125', '127', '130', '131', '137'],\n",
        " 'abdomen*': ['146', '147'],\n",
        " 'abilit*': ['355'],\n",
        " 'able*': ['355'],\n",
        " 'abortion*': ['146', '148', '149'],\n",
        " 'about': ['1', '16', '17'],\n",
        " 'above': ['1', '17', '252', '250'],\n",
        " 'abrupt*': ['253', '250'],\n",
        " 'abs': ['146', '147'],\n",
        " 'absent*': ['354'],\n",
        " 'absolute': ['131', '136'],\n",
        " 'absolutely': ['1', '16', '131', '136', '462'],\n",
        " 'abstain*': ['131', '137'],\n",
        " 'abuse*': ['125', '127', '129'],\n",
        " 'abusi*': ['125', '127', '129'],\n",
        " 'academ*': ['354'],\n",
        " 'accept': ['125', '126', '131', '132'],\n",
        " 'accepta*': ['125', '126', '131', '132'],\n",
        " 'accepted': ['11', '13', '125', '126', '131', '132'],\n",
        " 'accepting': ['125', '126', '131', '132'],\n",
        " 'accepts': ['125', '126', '131', '132'],\n",
        " 'accomplish*': ['354', '355'],\n",
        " 'account*': ['358'],\n",
        " 'accura*': ['131', '136'],\n",
        " 'ace': ['355'],\n",
        " 'ache*': ['125', '127', '130', '146', '148'],\n",
        " 'achiev*': ['354', '355'],\n",
        " 'aching': ['125', '127', '130', '146', '148'],\n",
        " 'acid*': ['140'],\n",
        " 'acknowledg*': ['131', '132'],\n",
        " 'acne': ['146', '148'],\n",
        " 'acquainta*': ['121', '123'],\n",
        " 'acquir*': ['355'],\n",
        " 'acquisition*': ['355'],\n",
        " 'acrid*': ['140'],\n",
        " 'across': ['1', '17', '252', '250'],\n",
        " 'act': ['250', '251'],\n",
        " 'action*': ['251', '250'],\n",
        " 'activat*': ['131', '133'],\n",
        " 'active*': ['125', '126'],\n",
        " 'actor*': ['356'],\n",
        " 'actress*': ['356'],\n",
        " 'actually': ['1', '16'],\n",
        " 'add': ['131', '138'],\n",
        " 'addict*': ['146', '148'],\n",
        " 'addit*': ['131', '138'],\n",
        " 'address': ['357'],\n",
        " 'adequa*': ['355'],\n",
        " 'adjust*': ['131', '132'],\n",
        " 'administrat*': ['354'],\n",
        " 'admir*': ['125', '126'],\n",
        " 'admit': ['11', '14', '121', '131', '132'],\n",
        " 'admits': ['11', '14', '121', '131', '132'],\n",
        " 'admitted': ['11', '13', '121', '131', '132'],\n",
        " 'admitting': ['121', '131', '132'],\n",
        " 'ador*': ['125', '126'],\n",
        " 'adult': ['121', '124'],\n",
        " 'adults': ['121', '124'],\n",
        " 'advanc*': ['251', '250', '355'],\n",
        " 'advantag*': ['125', '126', '355'],\n",
        " 'adventur*': ['125', '126'],\n",
        " 'advers*': ['125', '127'],\n",
        " 'advertising': ['354'],\n",
        " 'advice': ['121'],\n",
        " 'advil': ['146', '148'],\n",
        " 'advis*': ['121', '354'],\n",
        " 'aerobic*': ['356'],\n",
        " 'affair*': ['121'],\n",
        " 'affect': ['131', '133'],\n",
        " 'affected': ['11', '13', '131', '133'],\n",
        " 'affecting': ['131', '133'],\n",
        " 'affection*': ['125', '126'],\n",
        " 'affects': ['131', '133'],\n",
        " 'afraid': ['125', '127', '128'],\n",
        " 'after': ['1', '17', '253', '250'],\n",
        " 'afterlife*': ['253', '250', '359'],\n",
        " 'aftermath*': ['253', '250'],\n",
        " 'afternoon*': ['253', '250'],\n",
        " 'afterthought*': ['131', '132', '253', '250'],\n",
        " 'afterward*': ['253', '250'],\n",
        " 'again': ['1', '16', '253', '250'],\n",
        " 'against': ['1', '17'],\n",
        " 'age': ['253', '250'],\n",
        " 'aged': ['253', '250'],\n",
        " 'agent': ['354'],\n",
        " 'agents': ['354'],\n",
        " 'ages': ['253', '250'],\n",
        " 'aggravat*': ['125', '127', '129', '131', '133'],\n",
        " 'aggress*': ['125', '127', '129'],\n",
        " 'aging': ['253', '250'],\n",
        " 'agitat*': ['125', '127', '129'],\n",
        " 'agnost*': ['359'],\n",
        " 'ago': ['253', '250'],\n",
        " 'agoniz*': ['125', '127', '130'],\n",
        " 'agony': ['125', '127', '130'],\n",
        " 'agree': ['125', '126', '462'],\n",
        " 'agreeab*': ['125', '126'],\n",
        " 'agreed': ['125', '126'],\n",
        " 'agreeing': ['125', '126'],\n",
        " 'agreement*': ['125', '126'],\n",
        " 'agrees': ['125', '126'],\n",
        " 'ah': ['462'],\n",
        " 'ahead': ['1', '17', '253', '250', '355'],\n",
        " 'aids': ['146', '148', '149'],\n",
        " \"ain't\": ['11', '1', '12', '14', '19'],\n",
        " 'aint': ['11', '1', '12', '14', '19'],\n",
        " 'air': ['250', '252'],\n",
        " 'alarm*': ['125', '127', '128'],\n",
        " 'alcohol*': ['146', '148', '150'],\n",
        " 'alive': ['146', '148', '360'],\n",
        " 'all': ['1', '20', '131', '136'],\n",
        " 'alla': ['359'],\n",
        " 'allah*': ['359'],\n",
        " 'allerg*': ['146', '148'],\n",
        " 'allot': ['1', '20', '131', '135'],\n",
        " 'allow*': ['131', '133'],\n",
        " 'almost': ['131', '135'],\n",
        " 'alone': ['125', '127', '130'],\n",
        " 'along': ['1', '17', '131', '138'],\n",
        " 'alot': ['1', '10', '20', '131', '135'],\n",
        " 'already': ['253', '250'],\n",
        " 'alright*': ['125', '126', '462'],\n",
        " 'also': ['1', '16', '18'],\n",
        " 'altar*': ['359'],\n",
        " 'although': ['1', '18'],\n",
        " 'altogether': ['131', '136'],\n",
        " 'always': ['131', '136', '253', '250'],\n",
        " 'am': ['11', '1', '12', '14'],\n",
        " 'amaz*': ['125', '126'],\n",
        " 'ambigu*': ['131', '135'],\n",
        " 'ambiti*': ['354', '355'],\n",
        " 'amen': ['359'],\n",
        " 'amigo*': ['121', '123'],\n",
        " 'amish': ['359'],\n",
        " 'among*': ['1', '17', '252', '250'],\n",
        " 'amor*': ['125', '126'],\n",
        " 'amount*': ['20'],\n",
        " 'amput*': ['146', '148'],\n",
        " 'amus*': ['125', '126', '356'],\n",
        " 'an': ['1', '10'],\n",
        " 'anal': ['131', '137', '146', '147'],\n",
        " 'analy*': ['131', '132'],\n",
        " 'ancient*': ['253', '250'],\n",
        " 'and': ['1', '18', '131', '138'],\n",
        " 'angel': ['359'],\n",
        " 'angelic*': ['359'],\n",
        " 'angels': ['359'],\n",
        " 'anger*': ['125', '127', '129'],\n",
        " 'angr*': ['125', '127', '129'],\n",
        " 'anguish*': ['125', '127', '128'],\n",
        " 'ankle*': ['146', '147'],\n",
        " 'annoy*': ['125', '127', '129'],\n",
        " 'annual*': ['253', '250'],\n",
        " 'anorexi*': ['146', '148', '150'],\n",
        " 'another': ['1', '20'],\n",
        " 'answer*': ['131', '132'],\n",
        " 'antacid*': ['146', '148'],\n",
        " 'antagoni*': ['125', '127', '129'],\n",
        " 'antidepressant*': ['146', '148'],\n",
        " 'anus*': ['146', '147'],\n",
        " 'anxi*': ['125', '127', '128'],\n",
        " 'any': ['1', '20', '131', '135'],\n",
        " 'anybod*': ['1', '2', '9', '121', '131', '135'],\n",
        " 'anyhow': ['131', '135'],\n",
        " 'anymore': ['1', '20', '250', '253'],\n",
        " 'anyone*': ['1', '2', '9', '121', '131', '135'],\n",
        " 'anything': ['1', '2', '9', '131', '135'],\n",
        " 'anytime': ['131', '135', '253', '250'],\n",
        " 'anyway*': ['1', '16'],\n",
        " 'anywhere': ['1', '16', '131', '135', '252', '250'],\n",
        " 'aok': ['125', '126', '462'],\n",
        " 'apart': ['252', '250'],\n",
        " 'apartment*': ['356', '357'],\n",
        " 'apath*': ['125', '127'],\n",
        " 'apolog*': ['121'],\n",
        " 'appall*': ['125', '127'],\n",
        " 'apparent': ['131', '136'],\n",
        " 'apparently': ['1', '16', '131', '135'],\n",
        " 'appear': ['11', '14', '131', '135', '251', '250'],\n",
        " 'appeared': ['11', '13', '131', '135', '251', '250'],\n",
        " 'appearing': ['131', '135', '251', '250'],\n",
        " 'appears': ['11', '14', '131', '135', '251', '250'],\n",
        " 'appendic*': ['146', '148'],\n",
        " 'appendix': ['146', '147'],\n",
        " 'appeti*': ['146', '150'],\n",
        " 'applicant*': ['354'],\n",
        " 'applicat*': ['354'],\n",
        " 'appreciat*': ['125', '126', '131', '132'],\n",
        " 'apprehens*': ['125', '127', '128'],\n",
        " 'apprentic*': ['354'],\n",
        " 'approach*': ['251', '250'],\n",
        " 'approv*': ['355'],\n",
        " 'approximat*': ['131', '135'],\n",
        " 'april': ['253', '250'],\n",
        " 'arbitrar*': ['131', '135'],\n",
        " 'arch': ['146', '147'],\n",
        " 'are': ['11', '1', '12', '14'],\n",
        " 'area*': ['252', '250'],\n",
        " \"aren't\": ['11', '1', '12', '14', '19'],\n",
        " 'arent': ['11', '1', '12', '14', '19'],\n",
        " 'argh*': ['125', '127', '129'],\n",
        " 'argu*': ['121', '125', '127', '129'],\n",
        " 'arm': ['146', '147'],\n",
        " 'armies': ['121'],\n",
        " 'armpit*': ['146', '147'],\n",
        " 'arms*': ['146', '147'],\n",
        " 'army': ['121'],\n",
        " 'aroma*': ['140'],\n",
        " 'around': ['1', '16', '17', '131', '138', '252', '250'],\n",
        " 'arous*': ['146', '147', '149'],\n",
        " 'arrival*': ['251', '250'],\n",
        " 'arrive': ['11', '14', '251', '250'],\n",
        " 'arrived': ['11', '13', '251', '250'],\n",
        " 'arrives': ['11', '14', '251', '250'],\n",
        " 'arriving': ['251', '250'],\n",
        " 'arrogan*': ['125', '127', '129'],\n",
        " 'arse': ['146', '147', '22'],\n",
        " 'arsehole*': ['22'],\n",
        " 'arses': ['146', '147', '22'],\n",
        " 'art': ['356'],\n",
        " 'arter*': ['146', '147'],\n",
        " 'arthr*': ['146', '148'],\n",
        " 'artist*': ['356'],\n",
        " 'arts': ['356'],\n",
        " 'as': ['1', '17', '18'],\n",
        " 'asham*': ['125', '127', '128'],\n",
        " 'ask': ['11', '14', '121'],\n",
        " 'asked': ['11', '13', '121'],\n",
        " 'asking': ['121'],\n",
        " 'asks': ['11', '14', '121'],\n",
        " 'asleep': ['146', '147'],\n",
        " 'aspirin*': ['146', '148'],\n",
        " 'ass': ['146', '147', '149', '22'],\n",
        " 'assault*': ['125', '127', '129'],\n",
        " 'assembl*': ['121'],\n",
        " 'asses': ['146', '147', '149', '22'],\n",
        " 'asshole*': ['125', '127', '129', '22'],\n",
        " 'assign*': ['354'],\n",
        " 'assistan*': ['354'],\n",
        " 'associat*': ['354'],\n",
        " 'assum*': ['131', '132', '135'],\n",
        " 'assur*': ['125', '126', '131', '136'],\n",
        " 'asthma*': ['146', '148'],\n",
        " 'at': ['1', '17', '252', '250'],\n",
        " 'ate': ['11', '13', '146', '150'],\n",
        " 'athletic*': ['356'],\n",
        " 'atho': ['1', '18'],\n",
        " 'atm': ['358'],\n",
        " 'atms': ['358'],\n",
        " 'atop': ['1', '17', '252', '250'],\n",
        " 'attachment*': ['125', '126'],\n",
        " 'attack*': ['125', '127', '129'],\n",
        " 'attain*': ['355'],\n",
        " 'attempt*': ['355'],\n",
        " 'attend': ['251', '250'],\n",
        " 'attended': ['251', '250'],\n",
        " 'attending': ['251', '250'],\n",
        " 'attends': ['251', '250'],\n",
        " 'attent*': ['131', '132'],\n",
        " 'attract*': ['125', '126'],\n",
        " 'attribut*': ['131', '133'],\n",
        " 'auction*': ['358'],\n",
        " 'audibl*': ['140', '142'],\n",
        " 'audio*': ['140', '142'],\n",
        " 'audit': ['358'],\n",
        " 'audited': ['358'],\n",
        " 'auditing': ['358'],\n",
        " 'auditor': ['358'],\n",
        " 'auditorium*': ['354'],\n",
        " 'auditors': ['358'],\n",
        " 'audits': ['358'],\n",
        " 'august': ['253', '250'],\n",
        " 'aunt*': ['121', '122'],\n",
        " 'authorit*': ['355'],\n",
        " 'autops*': ['360'],\n",
        " 'autumn': ['253', '250'],\n",
        " 'aversi*': ['125', '127', '128'],\n",
        " 'avert*': ['131', '137'],\n",
        " 'avoid*': ['125', '127', '128', '131', '137'],\n",
        " 'aw': ['462'],\n",
        " 'award*': ['125', '126', '354', '355'],\n",
        " 'aware*': ['131', '132'],\n",
        " 'away': ['1', '17', '252', '250'],\n",
        " 'awesome': ['125', '126', '462'],\n",
        " 'awful': ['125', '127'],\n",
        " 'awhile': ['253', '250'],\n",
        " 'awkward*': ['125', '127', '128'],\n",
        " 'babe*': ['121', '124'],\n",
        " 'babies': ['121', '124'],\n",
        " 'baby*': ['121', '124'],\n",
        " 'back': ['1', '16', '253', '250'],\n",
        " 'backward*': ['252', '250'],\n",
        " 'backyard': ['357'],\n",
        " 'bad': ['125', '127'],\n",
        " 'bake*': ['146', '150', '357'],\n",
        " 'baking': ['146', '150', '357'],\n",
        " 'balcon*': ['357'],\n",
        " 'bald': ['146', '147'],\n",
        " 'ball': ['356'],\n",
        " 'ballet*': ['356'],\n",
        " 'bambino*': ['121', '124'],\n",
        " 'ban': ['131', '137'],\n",
        " 'band': ['121', '356'],\n",
        " 'bandage*': ['146', '148'],\n",
        " 'bandaid': ['146', '148'],\n",
        " 'bands': ['121', '356'],\n",
        " 'bank*': ['358'],\n",
        " 'banned': ['131', '137'],\n",
        " 'banning': ['131', '137'],\n",
        " 'bans': ['131', '137'],\n",
        " 'baptis*': ['359'],\n",
        " 'baptiz*': ['359'],\n",
        " 'bar': ['146', '150', '356'],\n",
        " 'barely': ['131', '135'],\n",
        " 'bargain*': ['358'],\n",
        " 'barrier*': ['131', '137'],\n",
        " 'bars': ['146', '150', '356'],\n",
        " 'baseball*': ['356'],\n",
        " 'based': ['131', '133'],\n",
        " 'bases': ['131', '133'],\n",
        " 'bashful*': ['125', '127'],\n",
        " 'basically': ['1', '16'],\n",
        " 'basis': ['131', '133'],\n",
        " 'basketball*': ['356'],\n",
        " 'bastard*': ['125', '127', '129', '22'],\n",
        " 'bath*': ['356', '357'],\n",
        " 'battl*': ['125', '127', '129'],\n",
        " 'be': ['11', '1', '12'],\n",
        " 'beach*': ['356'],\n",
        " 'beat': ['355'],\n",
        " 'beaten': ['125', '127', '129', '354', '355'],\n",
        " 'beaut*': ['125', '126', '140', '141'],\n",
        " 'became': ['11', '1', '12', '13', '131', '132'],\n",
        " 'because': ['1', '18', '131', '133'],\n",
        " 'become': ['11', '1', '12', '14', '131', '132'],\n",
        " 'becomes': ['11', '1', '12', '14', '131', '132'],\n",
        " 'becoming': ['11', '1', '12', '131', '132'],\n",
        " 'bed': ['357'],\n",
        " 'bedding': ['357'],\n",
        " 'bedroom*': ['357'],\n",
        " 'beds': ['357'],\n",
        " 'been': ['11', '1', '12', '13'],\n",
        " 'beer*': ['146', '150', '356'],\n",
        " 'before': ['1', '17', '253', '250'],\n",
        " 'began': ['11', '13', '253', '250'],\n",
        " 'beggar*': ['358'],\n",
        " 'begging': ['358'],\n",
        " 'begin': ['11', '14', '253', '250'],\n",
        " 'beginn*': ['253', '250'],\n",
        " 'begins': ['11', '14', '253', '250'],\n",
        " 'begun': ['253', '250'],\n",
        " 'behavio*': ['250', '251'],\n",
        " 'behind': ['1', '17'],\n",
        " 'being': ['11', '1', '12'],\n",
        " 'belief*': ['131', '132', '359'],\n",
        " 'believe': ['11', '14', '131', '132'],\n",
        " 'believed': ['11', '13', '131', '132'],\n",
        " 'believes': ['11', '14', '131', '132'],\n",
        " 'believing': ['131', '132'],\n",
        " 'bellies': ['146', '147'],\n",
        " 'belly': ['146', '147'],\n",
        " 'beloved': ['125', '126'],\n",
        " 'below': ['1', '17', '252', '250'],\n",
        " 'bend': ['252', '250'],\n",
        " 'bending': ['252', '250'],\n",
        " 'bends': ['252', '250'],\n",
        " 'beneath': ['1', '17', '252', '250'],\n",
        " 'benefic*': ['125', '126'],\n",
        " 'benefit': ['125', '126'],\n",
        " 'benefits': ['125', '126', '354'],\n",
        " 'benefitt*': ['125', '126'],\n",
        " 'benevolen*': ['125', '126'],\n",
        " 'benign*': ['125', '126'],\n",
        " 'bent': ['252', '250'],\n",
        " 'bereave*': ['360'],\n",
        " 'beside': ['1', '17', '252', '250'],\n",
        " 'besides': ['1', '17', '20', '131', '134'],\n",
        " 'best': ['125', '126', '355', '1', '20'],\n",
        " 'bet': ['131', '135', '358'],\n",
        " 'bets': ['131', '135', '358'],\n",
        " 'better': ['125', '126', '355'],\n",
        " 'betting': ['131', '135', '358'],\n",
        " 'between': ['1', '17'],\n",
        " 'beyond': ['1', '16', '17', '252', '250'],\n",
        " 'bf*': ['121', '123'],\n",
        " 'bi': ['146', '149'],\n",
        " 'biannu*': ['253', '250'],\n",
        " 'bible*': ['359'],\n",
        " 'biblic*': ['359'],\n",
        " 'bicep*': ['146', '147'],\n",
        " 'bicyc*': ['356'],\n",
        " 'big': ['252', '250'],\n",
        " 'bigger': ['252', '250'],\n",
        " 'biggest': ['252', '250'],\n",
        " 'bike*': ['356'],\n",
        " 'bill': ['358'],\n",
        " 'billed': ['358'],\n",
        " 'billing*': ['358'],\n",
        " 'billion*': ['1', '21'],\n",
        " 'bills': ['358'],\n",
        " 'bimonth*': ['253', '250'],\n",
        " 'binding': ['131', '137'],\n",
        " 'binge*': ['146', '148', '150'],\n",
        " 'binging': ['146', '148', '150'],\n",
        " 'biolog*': ['354'],\n",
        " 'bipolar': ['146', '148'],\n",
        " 'birdie*': ['356'],\n",
        " 'birth*': ['253', '250'],\n",
        " 'bishop*': ['359'],\n",
        " 'bit': ['20'],\n",
        " 'bitch*': ['125', '127', '129', '22'],\n",
        " 'bits': ['20'],\n",
        " 'bitter*': ['125', '127', '129', '140'],\n",
        " 'biweek*': ['253', '250'],\n",
        " 'biz': ['354'],\n",
        " 'black': ['140', '141'],\n",
        " 'blackboard*': ['354'],\n",
        " 'blacke*': ['140', '141'],\n",
        " 'blackish*': ['140', '141'],\n",
        " 'blackjack': ['356'],\n",
        " 'blacks': ['140', '141'],\n",
        " 'bladder*': ['146', '147'],\n",
        " 'blah': ['464'],\n",
        " 'blam*': ['121', '125', '127', '129'],\n",
        " 'blatant*': ['131', '136'],\n",
        " 'bldg*': ['354'],\n",
        " 'bleed*': ['146', '148'],\n",
        " 'blender*': ['357'],\n",
        " 'bless*': ['125', '126', '359'],\n",
        " 'blind*': ['146', '148', '140', '141'],\n",
        " 'block': ['131', '137'],\n",
        " 'blockbuster*': ['356'],\n",
        " 'blocked': ['131', '137'],\n",
        " 'blocker*': ['131', '137'],\n",
        " 'blocking': ['131', '137'],\n",
        " 'blocks': ['131', '137'],\n",
        " 'blog*': ['356'],\n",
        " 'blond*': ['140', '141'],\n",
        " 'blood': ['146', '147'],\n",
        " 'bloody': ['146', '147', '22'],\n",
        " 'blue*': ['140', '141'],\n",
        " 'blur*': ['131', '135'],\n",
        " 'bodi*': ['146', '147'],\n",
        " 'body*': ['146', '147'],\n",
        " 'boil*': ['146', '150'],\n",
        " 'bold*': ['125', '126'],\n",
        " 'bone': ['146', '147'],\n",
        " 'boner*': ['146', '149'],\n",
        " 'bones': ['146', '147'],\n",
        " 'bonus*': ['125', '126', '355', '358'],\n",
        " 'bony': ['146', '147'],\n",
        " 'boob*': ['146', '147', '149', '22'],\n",
        " 'book*': ['354', '356'],\n",
        " 'boom*': ['140', '142'],\n",
        " 'booz*': ['146', '150'],\n",
        " 'borderline*': ['131', '135'],\n",
        " 'bore*': ['125', '127'],\n",
        " 'boring': ['125', '127'],\n",
        " 'born': ['253', '250'],\n",
        " 'borrow*': ['358'],\n",
        " 'boss*': ['131', '133', '354'],\n",
        " 'both': ['1', '20', '131', '138', '252', '250'],\n",
        " 'bother*': ['125', '127', '129'],\n",
        " 'bottom*': ['252', '250'],\n",
        " 'bought': ['11', '13', '358'],\n",
        " 'bound*': ['131', '137'],\n",
        " 'bowel*': ['146', '147'],\n",
        " 'boy': ['121', '124'],\n",
        " \"boy's\": ['121', '124'],\n",
        " 'boyf*': ['121', '123'],\n",
        " 'boys*': ['121', '124'],\n",
        " 'brain*': ['146', '147'],\n",
        " 'brake*': ['131', '137'],\n",
        " 'brave*': ['125', '126'],\n",
        " 'bread': ['146', '150'],\n",
        " 'breadth': ['252', '250'],\n",
        " 'break': ['250'],\n",
        " 'breakfast*': ['146', '150'],\n",
        " 'breast*': ['146', '147', '149'],\n",
        " 'breath*': ['146', '147'],\n",
        " 'bridle*': ['131', '137'],\n",
        " 'brief*': ['251', '250'],\n",
        " 'bright*': ['125', '126', '140', '141'],\n",
        " 'brillian*': ['125', '126'],\n",
        " 'bring': ['11', '14', '251', '250'],\n",
        " 'bringing': ['251', '250'],\n",
        " 'brings': ['11', '14', '251', '250'],\n",
        " 'brink': ['252', '250'],\n",
        " 'bro': ['121', '122'],\n",
        " 'broad*': ['252', '250'],\n",
        " 'broke': ['125', '127', '130'],\n",
        " 'broker*': ['354', '358'],\n",
        " 'bronchi*': ['146', '148'],\n",
        " 'broom*': ['357'],\n",
        " 'bros': ['121', '122'],\n",
        " 'brother*': ['121', '122'],\n",
        " 'brought': ['11', '13', '251', '250'],\n",
        " 'brown*': ['140', '141'],\n",
        " 'brunch*': ['146', '150'],\n",
        " 'brush*': ['140', '143'],\n",
        " 'brutal*': ['125', '127', '129'],\n",
        " 'buck': ['358'],\n",
        " 'bucks': ['358'],\n",
        " 'bud': ['121', '123'],\n",
        " 'buddh*': ['359'],\n",
        " 'buddies*': ['121', '123'],\n",
        " 'buddy*': ['121', '123'],\n",
        " 'budget*': ['358'],\n",
        " 'building': ['250'],\n",
        " 'bulimi*': ['146', '148', '150'],\n",
        " 'bunch': ['1', '20'],\n",
        " 'burden*': ['125', '127'],\n",
        " 'bureau*': ['354'],\n",
        " 'burial*': ['360'],\n",
        " 'buried': ['360'],\n",
        " 'burnout*': ['354', '355'],\n",
        " 'burp*': ['146', '148'],\n",
        " 'bury': ['360'],\n",
        " 'business*': ['354', '358'],\n",
        " 'busy': ['250', '253', '354'],\n",
        " 'but': ['1', '18', '131', '139'],\n",
        " 'butt': ['146', '147', '149', '22'],\n",
        " \"butt's\": ['146', '147', '149', '22'],\n",
        " 'butter*': ['140'],\n",
        " 'butts': ['146', '147', '149', '22'],\n",
        " 'buy*': ['358'],\n",
        " 'by': ['1', '17'],\n",
        " 'bye': ['121', '250', '253'],\n",
        " 'caf*': ['146', '150'],\n",
        " 'calculus': ['354'],\n",
        " 'call': ['121'],\n",
        " 'called': ['11', '13', '121'],\n",
        " 'caller*': ['121'],\n",
        " 'calling': ['121'],\n",
        " 'calls': ['121'],\n",
        " 'calm*': ['125', '126'],\n",
        " 'came': ['11', '13', '131', '138', '251', '250'],\n",
        " 'camping': ['356'],\n",
        " 'campus*': ['354'],\n",
        " 'can': ['11', '1', '12', '14'],\n",
        " \"can't\": ['11', '1', '12', '14', '19'],\n",
        " 'cancer*': ['146', '148'],\n",
        " 'candie*': ['146', '150'],\n",
        " 'candle*': ['140', '141'],\n",
        " 'candy': ['146', '150'],\n",
        " 'cannot': ['11', '1', '12', '14', '19'],\n",
        " 'cant': ['11', '1', '12', '14', '19'],\n",
        " 'capab*': ['355'],\n",
        " 'capacit*': ['252', '250'],\n",
        " 'captain': ['121'],\n",
        " 'car': ['250', '251'],\n",
        " 'caramel*': ['140'],\n",
        " 'cardia*': ['146', '148'],\n",
        " 'cardio*': ['146', '148'],\n",
        " 'cards': ['356'],\n",
        " 'care': ['11', '14', '125', '126'],\n",
        " 'cared': ['11', '13', '125', '126'],\n",
        " 'career*': ['354'],\n",
        " 'carefree': ['125', '126'],\n",
        " 'careful*': ['125', '126', '131', '137'],\n",
        " 'careless*': ['125', '127'],\n",
        " 'cares': ['11', '14', '125', '126'],\n",
        " 'caress*': ['140', '143'],\n",
        " 'caring': ['125', '126'],\n",
        " 'carpet*': ['357'],\n",
        " 'carried': ['11', '13', '251', '250'],\n",
        " 'carrier*': ['251', '250'],\n",
        " 'carries': ['11', '14', '251', '250'],\n",
        " 'carry': ['11', '14', '251', '250'],\n",
        " 'carrying': ['251', '250'],\n",
        " 'cash*': ['358'],\n",
        " 'casino*': ['356', '358'],\n",
        " 'casket*': ['360'],\n",
        " 'casual': ['125', '126', '356'],\n",
        " 'casually': ['125', '126'],\n",
        " 'casualt*': ['360'],\n",
        " 'catch': ['250', '251'],\n",
        " 'categor*': ['131', '132'],\n",
        " 'catholic*': ['359'],\n",
        " 'caught': ['11', '13', '250', '251'],\n",
        " 'caus*': ['131', '133'],\n",
        " 'caut*': ['131', '137'],\n",
        " 'cd*': ['356'],\n",
        " 'cease*': ['131', '137', '253', '250'],\n",
        " 'ceasing': ['131', '137', '253', '250'],\n",
        " 'ceiling*': ['252', '250'],\n",
        " 'celebrat*': ['121', '355', '356'],\n",
        " 'celebrit*': ['356'],\n",
        " 'cell': ['121'],\n",
        " 'cellphon*': ['121'],\n",
        " 'cells': ['121'],\n",
        " 'cellular*': ['121'],\n",
        " 'cemet*': ['360'],\n",
        " 'cent': ['358'],\n",
        " 'center*': ['252', '250'],\n",
        " 'centre*': ['252', '250'],\n",
        " 'cents': ['358'],\n",
        " 'centur*': ['253', '250'],\n",
        " 'ceo*': ['354'],\n",
        " 'certain*': ['125', '126', '131', '136'],\n",
        " 'certif*': ['354'],\n",
        " 'cetera': ['1', '20'],\n",
        " 'chairm*': ['354'],\n",
        " 'chalk': ['354'],\n",
        " 'challeng*': ['125', '126', '354', '355'],\n",
        " 'champ*': ['125', '126', '354', '355'],\n",
        " 'chance': ['131', '135'],\n",
        " 'change': ['250', '251', '131', '133'],\n",
        " 'changed': ['11', '13', '250', '251', '131', '133'],\n",
        " 'changes': ['250', '251', '131', '133'],\n",
        " 'changing': ['250', '251', '131', '133'],\n",
        " 'channel*': ['356'],\n",
        " 'chapel*': ['359'],\n",
        " 'chaplain*': ['359'],\n",
        " 'charit*': ['125', '126', '358'],\n",
        " 'charm*': ['125', '126'],\n",
        " 'chat*': ['121', '356'],\n",
        " 'cheap*': ['358'],\n",
        " 'cheat*': ['125', '127', '129'],\n",
        " 'check': ['358'],\n",
        " 'checkers': ['356'],\n",
        " 'checking': ['358'],\n",
        " 'checks': ['358'],\n",
        " 'checkup*': ['146', '148'],\n",
        " 'cheek*': ['146', '147'],\n",
        " 'cheer*': ['125', '126'],\n",
        " 'chequ*': ['358'],\n",
        " 'cherish*': ['125', '126'],\n",
        " 'chess': ['356'],\n",
        " 'chest*': ['146', '147'],\n",
        " 'chew*': ['146', '150'],\n",
        " 'chick': ['121', '124'],\n",
        " \"chick'*\": ['121', '124'],\n",
        " 'child': ['121', '124'],\n",
        " \"child's\": ['121', '124'],\n",
        " 'childhood': ['250', '253'],\n",
        " 'children*': ['121', '124'],\n",
        " 'chillin*': ['356'],\n",
        " 'chills': ['146', '148'],\n",
        " 'chiropract*': ['146', '148'],\n",
        " 'chlamydia': ['146', '148', '149'],\n",
        " 'chocolate*': ['140'],\n",
        " 'choice*': ['131', '132'],\n",
        " 'choir*': ['356', '140', '142'],\n",
        " 'chok*': ['146', '148'],\n",
        " 'cholester*': ['146', '148'],\n",
        " 'choos*': ['131', '132'],\n",
        " 'chore*': ['357'],\n",
        " 'chorus': ['356'],\n",
        " 'chow*': ['146', '150'],\n",
        " 'christ': ['359'],\n",
        " 'christian*': ['359'],\n",
        " 'christmas*': ['250', '253', '359'],\n",
        " 'chronic*': ['146', '148'],\n",
        " 'chuckl*': ['125', '126'],\n",
        " 'church*': ['359'],\n",
        " 'cigar*': ['146', '150'],\n",
        " 'cinema*': ['356'],\n",
        " 'circle': ['140', '141'],\n",
        " 'citizen': ['121', '124'],\n",
        " \"citizen'*\": ['121', '124'],\n",
        " 'citizens': ['121', '124'],\n",
        " 'citrus*': ['140'],\n",
        " 'city': ['250', '252'],\n",
        " 'clarif*': ['131', '132'],\n",
        " 'class': ['354'],\n",
        " 'classes': ['354'],\n",
        " 'classmate*': ['354'],\n",
        " 'classroom*': ['354'],\n",
        " 'clean*': ['357'],\n",
        " 'clear': ['131', '136'],\n",
        " 'clearly': ['1', '16', '131', '136'],\n",
        " 'clergy': ['359'],\n",
        " 'clever*': ['125', '126'],\n",
        " 'click*': ['140', '141'],\n",
        " 'climb*': ['251', '250', '355'],\n",
        " 'clinic*': ['146', '148'],\n",
        " 'clock*': ['253', '250'],\n",
        " 'close': ['131', '138', '252', '250'],\n",
        " 'closed': ['252', '250'],\n",
        " 'closely': ['251', '250'],\n",
        " 'closer': ['252', '250'],\n",
        " 'closes': ['251', '250'],\n",
        " 'closest': ['252', '250'],\n",
        " 'closet': ['357'],\n",
        " 'closets': ['357'],\n",
        " 'closing': ['251', '250'],\n",
        " 'closure': ['131', '132', '355'],\n",
        " 'clothes': ['146', '147'],\n",
        " 'club*': ['356'],\n",
        " 'coach*': ['356'],\n",
        " 'cock': ['146', '147', '149', '22'],\n",
        " 'cocks*': ['146', '147', '149', '22'],\n",
        " 'cocktail*': ['146', '150', '356'],\n",
        " 'codeine': ['146', '148'],\n",
        " 'coffee*': ['146', '150', '356'],\n",
        " 'coffin*': ['360'],\n",
        " 'cohere*': ['131', '132'],\n",
        " 'coin': ['358'],\n",
        " 'coins': ['358'],\n",
        " 'coke*': ['146', '150'],\n",
        " 'cold*': ['140', '143'],\n",
        " 'collab*': ['354'],\n",
        " 'colleague*': ['121', '123', '354'],\n",
        " 'colleg*': ['354'],\n",
        " 'cologne*': ['140'],\n",
        " 'colon': ['146', '147'],\n",
        " 'colono*': ['146', '148'],\n",
        " 'colons': ['146', '147'],\n",
        " 'color*': ['140', '141'],\n",
        " 'colour*': ['140', '141'],\n",
        " 'column*': ['140', '141'],\n",
        " 'com': ['354'],\n",
        " 'coma*': ['146', '148'],\n",
        " 'come': ['11', '14', '131', '138', '251', '250'],\n",
        " 'comed*': ['125', '126', '356'],\n",
        " 'comes': ['11', '14', '251', '250'],\n",
        " 'comfort*': ['125', '126'],\n",
        " 'comic*': ['356'],\n",
        " 'coming': ['251', '250'],\n",
        " 'comment*': ['121'],\n",
        " 'commerc*': ['354'],\n",
        " 'commit': ['131', '136'],\n",
        " 'commitment*': ['125', '126', '131', '136'],\n",
        " 'commits': ['131', '136'],\n",
        " 'committ*': ['131', '136'],\n",
        " 'common': ['250', '253'],\n",
        " 'commun*': ['121'],\n",
        " 'commute*': ['354'],\n",
        " 'commuting': ['354'],\n",
        " 'companies': ['354'],\n",
        " 'companion': ['121', '123'],\n",
        " 'companions': ['121', '123'],\n",
        " 'companionship*': ['121'],\n",
        " 'company': ['354'],\n",
        " 'compassion*': ['121', '125', '126'],\n",
        " 'compel*': ['131', '133'],\n",
        " 'compensat*': ['358'],\n",
        " 'compet*': ['355'],\n",
        " 'complain*': ['121', '125', '127'],\n",
        " 'complete': ['131', '136'],\n",
        " 'completed': ['131', '136'],\n",
        " 'completely': ['1', '16', '131', '136'],\n",
        " 'completes': ['131', '136'],\n",
        " 'complex*': ['131', '132'],\n",
        " 'compliance': ['131', '133'],\n",
        " 'complica*': ['131', '132'],\n",
        " 'complie*': ['131', '133'],\n",
        " 'compliment*': ['125', '126'],\n",
        " 'comply*': ['131', '133'],\n",
        " 'compreh*': ['131', '132'],\n",
        " 'compulsiv*': ['131', '137'],\n",
        " 'comput*': ['354'],\n",
        " 'comrad*': ['121', '123'],\n",
        " 'concentrat*': ['131', '132'],\n",
        " 'concerned': ['125'],\n",
        " 'concert*': ['356', '140', '142'],\n",
        " 'conclud*': ['131', '132', '133', '355'],\n",
        " 'conclus*': ['131', '132', '355'],\n",
        " 'condo': ['357'],\n",
        " 'condom': ['146', '149'],\n",
        " 'condominium*': ['357'],\n",
        " 'condoms': ['146', '149'],\n",
        " 'condos': ['357'],\n",
        " 'conferenc*': ['354'],\n",
        " 'confess*': ['121', '131', '132', '359'],\n",
        " 'confide': ['121'],\n",
        " 'confided': ['121'],\n",
        " 'confidence': ['125', '126', '131', '136', '355'],\n",
        " 'confident': ['125', '126', '131', '136', '355'],\n",
        " 'confidently': ['125', '126', '131', '136', '355'],\n",
        " 'confides': ['121'],\n",
        " 'confiding': ['121'],\n",
        " 'confin*': ['131', '137'],\n",
        " 'conflict*': ['131', '137'],\n",
        " 'confront*': ['125', '127', '129'],\n",
        " 'confus*': ['125', '127', '128', '131', '135'],\n",
        " 'congest*': ['146', '148'],\n",
        " 'conglom*': ['354'],\n",
        " 'congregat*': ['121'],\n",
        " 'connection*': ['250', '252'],\n",
        " 'conquer*': ['355'],\n",
        " 'conscientious*': ['355'],\n",
        " 'conscious*': ['131', '132'],\n",
        " 'consequen*': ['131', '133'],\n",
        " 'conserv*': ['131', '137'],\n",
        " 'consider': ['131', '132'],\n",
        " 'considerate': ['125', '126'],\n",
        " 'considered': ['131', '132'],\n",
        " 'considering': ['131', '132'],\n",
        " 'considers': ['131', '132'],\n",
        " 'constant': ['250', '253'],\n",
        " 'constantly': ['1', '16', '250', '253'],\n",
        " 'constipat*': ['146', '148'],\n",
        " 'constrain*': ['131', '137'],\n",
        " 'constrict*': ['131', '137'],\n",
        " 'consult*': ['121', '354'],\n",
        " 'consumer*': ['354', '358'],\n",
        " 'contact*': ['121'],\n",
        " 'contag*': ['146', '148'],\n",
        " 'contain*': ['131', '137'],\n",
        " 'contemplat*': ['131', '132'],\n",
        " 'contempt*': ['125', '127', '129'],\n",
        " 'contented*': ['125', '126'],\n",
        " 'contentment': ['125', '126'],\n",
        " 'contingen*': ['131', '135'],\n",
        " 'continu*': ['253', '250'],\n",
        " 'contracts': ['354'],\n",
        " 'contradic*': ['121', '125', '127', '129', '131', '137'],\n",
        " 'control*': ['131', '133', '137', '355'],\n",
        " 'convent': ['359'],\n",
        " 'convents': ['359'],\n",
        " 'convers*': ['121'],\n",
        " 'convinc*': ['125', '126'],\n",
        " 'cook*': ['146', '150', '356'],\n",
        " 'cool': ['125', '126', '462', '140', '143'],\n",
        " 'cornea*': ['146', '147'],\n",
        " 'corner': ['252', '250'],\n",
        " 'corners': ['252', '250'],\n",
        " 'coronar*': ['146', '148'],\n",
        " 'coroner*': ['360', '360'],\n",
        " 'corp': ['354'],\n",
        " 'corporat*': ['354', '358'],\n",
        " 'corps': ['354'],\n",
        " 'corpse*': ['360'],\n",
        " 'correct*': ['131', '136'],\n",
        " 'correlat*': ['131', '132'],\n",
        " 'cos': ['131', '133'],\n",
        " 'cost*': ['358'],\n",
        " 'couch*': ['357'],\n",
        " 'cough*': ['146', '148'],\n",
        " 'could': ['11', '1', '12', '131', '134'],\n",
        " \"could've\": ['11', '1', '12', '13', '15', '131', '134'],\n",
        " \"couldn't\": ['11', '1', '12', '19', '131', '134'],\n",
        " 'couldnt': ['11', '1', '12', '19', '131', '134'],\n",
        " 'couldve': ['11', '1', '12', '13', '15', '131', '134'],\n",
        " 'counc*': ['121', '354'],\n",
        " 'couns*': ['121', '354'],\n",
        " 'countr*': ['250', '252'],\n",
        " 'couple': ['1', '20'],\n",
        " 'coupon*': ['358'],\n",
        " 'courag*': ['125', '126'],\n",
        " 'course*': ['354'],\n",
        " 'cousin*': ['121', '122'],\n",
        " 'coverage': ['250', '252'],\n",
        " 'coworker*': ['121', '354'],\n",
        " 'coz': ['131', '133'],\n",
        " 'cramp*': ['146', '148'],\n",
        " 'crap': ['125', '127', '129', '146', '147', '22'],\n",
        " 'crappy': ['125', '127', '129', '22'],\n",
        " 'craz*': ['125', '127', '128'],\n",
        " 'cream': ['140', '141'],\n",
        " 'create*': ['125', '126', '131', '133', '355'],\n",
        " 'creati*': ['125', '126', '131', '133', '355'],\n",
        " 'credential*': ['354'],\n",
        " 'credit*': ['125', '126', '354', '358'],\n",
        " 'cremat*': ['360'],\n",
        " 'cried': ['11', '13', '125', '127', '130'],\n",
        " 'cries': ['125', '127', '130'],\n",
        " 'critical': ['125', '127', '129'],\n",
        " 'critici*': ['125', '127', '129'],\n",
        " 'cross*': ['251', '250'],\n",
        " 'crotch': ['146', '147'],\n",
        " 'crowd*': ['121'],\n",
        " 'crown*': ['355'],\n",
        " 'crucifi*': ['359'],\n",
        " 'crude*': ['125', '127', '129'],\n",
        " 'cruel*': ['125', '127', '129'],\n",
        " 'cruis*': ['251', '250', '356'],\n",
        " 'crusade*': ['359'],\n",
        " 'crushed': ['125', '127', '130'],\n",
        " 'cry': ['125', '127', '130'],\n",
        " 'crying': ['125', '127', '130'],\n",
        " 'crypt*': ['360'],\n",
        " 'cubicle*': ['354'],\n",
        " 'cuddl*': ['146', '149'],\n",
        " 'cultur*': ['121'],\n",
        " 'cunt*': ['125', '127', '129', '22'],\n",
        " 'curb*': ['131', '137'],\n",
        " 'curio*': ['131', '132'],\n",
        " 'currenc*': ['358'],\n",
        " 'current*': ['253', '250'],\n",
        " 'curricul*': ['354'],\n",
        " 'curtail*': ['131', '137'],\n",
        " 'curtain*': ['357'],\n",
        " 'customer*': ['354', '358'],\n",
        " 'cut': ['125', '127', '129'],\n",
        " 'cute*': ['125', '126'],\n",
        " 'cutie*': ['125', '126'],\n",
        " 'cuz': ['1', '18', '131', '133'],\n",
        " 'cv*': ['354'],\n",
        " 'cycle*': ['253', '250'],\n",
        " 'cynic': ['125', '127', '129'],\n",
        " 'cyst*': ['146', '148'],\n",
        " 'dad*': ['121', '122'],\n",
        " 'dail*': ['253', '250'],\n",
        " 'damag*': ['125', '127', '130'],\n",
        " 'damn*': ['125', '127', '129', '22'],\n",
        " 'danc*': ['251', '250', '356'],\n",
        " 'dang': ['22'],\n",
        " 'danger*': ['125', '127', '129'],\n",
        " 'daring': ['125', '126'],\n",
        " 'darlin*': ['125', '126'],\n",
        " 'darn': ['22'],\n",
        " 'date*': ['253', '250'],\n",
        " 'dating': ['121'],\n",
        " 'daughter*': ['121', '122'],\n",
        " 'day*': ['253', '250'],\n",
        " 'daze*': ['125', '127'],\n",
        " 'dead': ['360'],\n",
        " 'deadline*': ['354'],\n",
        " 'deaf*': ['146', '148', '140', '142'],\n",
        " 'deal': ['121'],\n",
        " 'dean*': ['354'],\n",
        " 'dear*': ['125', '126'],\n",
        " 'death*': ['360'],\n",
        " 'debit*': ['358'],\n",
        " 'debt*': ['358'],\n",
        " 'decade*': ['253', '250'],\n",
        " 'decay*': ['125', '127', '253', '250'],\n",
        " 'decease*': ['360'],\n",
        " 'december': ['253', '250'],\n",
        " 'decid*': ['131', '132'],\n",
        " 'decis*': ['131', '132'],\n",
        " 'decongest*': ['146', '148'],\n",
        " 'decorat*': ['356'],\n",
        " 'deduc*': ['131', '132', '133'],\n",
        " 'deep*': ['252', '250'],\n",
        " 'defeat*': ['125', '127', '130', '355'],\n",
        " 'defect*': ['125', '127'],\n",
        " 'defenc*': ['125', '127', '129', '131', '137'],\n",
        " 'defens*': ['125', '127', '129', '131', '137'],\n",
        " 'define': ['131', '132'],\n",
        " 'defined': ['131', '136'],\n",
        " 'defines': ['131', '132'],\n",
        " 'defining': ['131', '132'],\n",
        " 'definite': ['125', '126', '131', '136'],\n",
        " 'definitely': ['1', '16', '125', '126', '131', '136'],\n",
        " 'definitive*': ['131', '136'],\n",
        " 'degrad*': ['125', '127'],\n",
        " 'delay*': ['131', '137', '253', '250'],\n",
        " 'delectabl*': ['140', '125', '126'],\n",
        " 'delegat*': ['354'],\n",
        " 'delicate*': ['125', '126'],\n",
        " 'delicious*': ['125', '126', '140'],\n",
        " 'deligh*': ['125', '126'],\n",
        " 'deliver*': ['251', '250'],\n",
        " 'demise': ['360'],\n",
        " 'demon*': ['359'],\n",
        " 'demote*': ['354'],\n",
        " 'den': ['357'],\n",
        " 'denia*': ['131', '137'],\n",
        " 'denie*': ['131', '137'],\n",
        " 'dense': ['252', '250'],\n",
        " 'densit*': ['252', '250'],\n",
        " 'dentist*': ['146', '148'],\n",
        " 'deny*': ['131', '137'],\n",
        " 'deoder*': ['140'],\n",
        " 'depart': ['251', '250'],\n",
        " 'departed': ['251', '250'],\n",
        " 'departing': ['251', '250'],\n",
        " 'department*': ['354'],\n",
        " 'departs': ['251', '250'],\n",
        " 'departure*': ['251', '250'],\n",
        " 'depend': ['131', '133', '135'],\n",
        " 'depended': ['11', '13', '131', '133', '135'],\n",
        " 'depending': ['131', '133', '135'],\n",
        " 'depends': ['11', '14', '131', '133', '135'],\n",
        " 'deposit*': ['358'],\n",
        " 'depress*': ['125', '127', '130'],\n",
        " 'depriv*': ['125', '127', '130'],\n",
        " 'dept': ['354'],\n",
        " 'depth*': ['252', '250'],\n",
        " 'derma*': ['146', '148'],\n",
        " 'describe': ['11', '14', '121'],\n",
        " 'described': ['11', '13', '121'],\n",
        " 'describes': ['11', '14', '121'],\n",
        " 'describing': ['121'],\n",
        " 'desir*': ['131', '134'],\n",
        " 'desk*': ['354'],\n",
        " 'despair*': ['125', '127', '130'],\n",
        " 'desperat*': ['125', '127', '128'],\n",
        " 'despis*': ['125', '127', '129'],\n",
        " 'despite': ['1', '17'],\n",
        " 'dessert*': ['146', '150'],\n",
        " 'destroy*': ['125', '127', '129'],\n",
        " 'destruct*': ['125', '127', '129'],\n",
        " 'determina*': ['125', '126', '131', '132', '355'],\n",
        " 'determine': ['131', '132'],\n",
        " 'determined': ['125', '126', '131', '132', '355'],\n",
        " 'determines': ['131', '132'],\n",
        " 'determining': ['131', '132'],\n",
        " 'detox*': ['146', '148'],\n",
        " 'devastat*': ['125', '127', '130'],\n",
        " 'devil*': ['125', '127', '359'],\n",
        " 'devot*': ['125', '126'],\n",
        " 'diabet*': ['146', '148'],\n",
        " 'diagnos*': ['146', '148'],\n",
        " 'diagonal*': ['252', '250'],\n",
        " ...}"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' Returns frequency of function words '''\n",
      "def get_func_word_freq(words,funct_words):\n",
      "    fdist = nltk.FreqDist([funct_word for funct_word in funct_words if funct_word in words]) \n",
      "    funct_freq = {}    \n",
      "    for key,value in fdist.iteritems():\n",
      "        funct_freq[key] = value\n",
      "    return funct_freq\n",
      "\n",
      "liwc_dict_file='liwc_file.txt'\n",
      "\n",
      "''' Read LIWC 2007 English dictionary and extract function words '''\n",
      "def load_liwc_funct():\n",
      "    funct_words = set()\n",
      "    data_file = open(liwc_dict_file, 'rb')\n",
      "    lines = data_file.readlines()\n",
      "    for line in lines:\n",
      "        row = line.rstrip().split(\"\\t\")\n",
      "        if '1' in row:\n",
      "            if row[0][-1:] == '*' :\n",
      "                funct_words.add(row[0][:-1])\n",
      "            else :\n",
      "                funct_words.add(row[0])\n",
      "    return list(funct_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    }
   ],
   "metadata": {}
  }
 ]
}