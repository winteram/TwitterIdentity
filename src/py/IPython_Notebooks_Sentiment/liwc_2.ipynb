{
 "metadata": {
  "name": "",
  "signature": "sha256:5183798957766123d8b885612e01ef4c165db39dc0ed52e394f4e505d176b3f7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import sys, os\n",
      "import pandas as pd\n",
      "import cPickle as pickle\n",
      "import re\n",
      "import numpy as np\n",
      "import nltk\n",
      " \n",
      "\n",
      "os.chdir('C:/Users/Asaf/Documents/GitHub/TwitterIdentity/src/py')\n",
      "\n",
      "\n",
      "\n",
      "survey=pd.read_pickle('DataFrames/SurveySentiment')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "survey.keys()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "Index([u'Agree', u'FBid', u'IUname', u'Id', u'Referred_by', u'Twitid', u'active', u'afraid', u'alert', u'ashamed', u'attentive', u'comments', u'con_agree_0', u'con_agree_1', u'con_agree_10', u'con_agree_11', u'con_agree_12', u'con_agree_13', u'con_agree_14', u'con_agree_15', u'con_agree_16', u'con_agree_17', u'con_agree_18', u'con_agree_19', u'con_agree_2', u'con_agree_20', u'con_agree_21', u'con_agree_22', u'con_agree_23', u'con_agree_24', u'con_agree_25', u'con_agree_26', u'con_agree_27', u'con_agree_28', u'con_agree_29', u'con_agree_3', u'con_agree_30', u'con_agree_31', u'con_agree_32', u'con_agree_33', u'con_agree_34', u'con_agree_4', u'con_agree_5', u'con_agree_6', u'con_agree_7', u'con_agree_8', u'con_agree_9', u'determined', u'distressed', u'edu', u'ended', u'enthusiastic', u'ethnicity', u'excited', u'fam_income', u'fb_academic', u'fb_academic_comments', u'fb_appearance', u'fb_appearance_comments', u'fb_comments', u'fb_doing', u'fb_doing_comments', u'fb_entertain', u'fb_entertain_comments', u'fb_family', u'fb_family_comments', u'fb_feel', u'fb_feel_comments', u'fb_god', u'fb_god_comments', u'fb_political', u'fb_political_comments', u'fb_where', u'fb_where_comments', u'gender', u'guilty', u'hostile', u'income', u'inspired', u'interested', u'irritable', u'jittery', u'nervous', u'own_URL1', u'own_URL2', u'own_URL3', u'own_form11', u'own_form12', u'own_form21', u'own_form22', u'own_form31', u'own_form32', u'party', u'party0_bond', u'party10_common_avg', u'party11_similar_avg', u'party12_common_oth', u'party13_similar_oth', u'party1_solidarity', u'party2_committed', ...], dtype='object')"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import sys, os\n",
      "import pandas as pd\n",
      "import cPickle as pickle\n",
      "import re\n",
      "import numpy as np\n",
      "import nltk\n",
      " \n",
      "\n",
      "os.chdir('C:/Users/Asaf/Documents/GitHub/TwitterIdentity/src/py')\n",
      "\n",
      "\n",
      "\n",
      "survey=pd.read_pickle('DataFrames/SurveySentiment')\n",
      "\n",
      "\n",
      "import string \n",
      "\n",
      "\n",
      "#This function creates two dictionaries one for categories, like pronounds posneg emoions and an associated id number, and the other dictionary\n",
      "#is, returned in the function as \"dic\", are the words Liwc uses and the category ids associated with them\n",
      "def read_liwc(filename):\n",
      "    liwc_data = open(filename, 'r')\n",
      "    \n",
      "    mode = 0\n",
      "    cat = {}\n",
      "    dic = {}\n",
      "\n",
      "    for line in liwc_data:\n",
      "        line = line.strip('\\r\\n')\n",
      "        if line == '%':\n",
      "            mode += 1\n",
      "            continue\n",
      "        elif mode == 1: # cat\n",
      "            chunks = line.split('\\t')\n",
      "            cat[chunks[0]] = chunks[1]\n",
      "        elif mode == 2: # dic\n",
      "            chunks = line.split('\\t')\n",
      "            word = chunks.pop(0)\n",
      "            dic[word] = chunks\n",
      "    return (cat, dic)\n",
      "survey['tw_liwc']=None #Initialize column for Liwc scores.\n",
      "survey['tw_tot_words']=None \n",
      "#cat gives us the dictionaries of words that we are looking for and their associated categories, cat[1], as well as the Liwc categories\n",
      "#\n",
      "cat=read_liwc('liwc_file.txt')\n",
      "\n",
      "tw_ids=os.listdir('B:/Twitter_Part')\n",
      "path='B:/Twitter_Part/'\n",
      "error_log=[]\n",
      "\n",
      " # take every tweet's sentiment and combine them all. \n",
      "\n",
      "\n",
      "for j in tw_ids[0:2]:\n",
      "\n",
      "    sent_statuses=''# This is just the collection of all statuses. \n",
      "    filename=path+str(j)+'/tweets' #The tweets we will pull\n",
      "    \n",
      "\n",
      "\n",
      "    z=pickle.load(open(filename))\n",
      "    #sent_path=path+str(j)+'/LIWC' #In case I want to maybe pickle Liwc scores for each person, actually this seems inefficient.\n",
      "    for status in z:\n",
      "        if 'text' in status.keys(): #Sometimes there is no text, so it's important to specify\n",
      "            if status['text'] != 'None': #I believe sometimes it is actually None, anyway, it doesn't hurt to have this\n",
      "                message=status['text'].encode('utf-8') #The enconding seems to increase the amount I can retrieve that appear like readable characters\n",
      "\n",
      "\n",
      "            \n",
      "\n",
      "                # Features- Takes in a text string, tokenizes it using nltk, also, make it lower case. \n",
      "                # Two ways to to do it. Just, agregate all of a person's info- yeah, just agregate for now into one big string\n",
      "\n",
      "                sent_statuses+= ' '+ message\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    sent_statuses=nltk.word_tokenize(sent_statuses.lower()) # After the encoding I \n",
      "    freq=nltk.FreqDist(sent_statuses) #Just use nltk to do the work of creating the frequency distribution\n",
      "    words=freq.keys() #Gives us each word\n",
      "    score={}\n",
      "    score2={}\n",
      "    for entry in cat[0].keys(): #Go through each category, like \"work\", \"posneg emotions\", etc\n",
      "        score[entry]=0 #In the new dictionary, \"score\" just initialize a key for that category, with a value of 0\n",
      "        \n",
      " #for now I can add up all all the words used in Tweets- but it won't be entirely accurate\n",
      "    #because of the format and the way things were tokenized. I can do relative scores for distributions. \n",
      "\n",
      "    for i in cat[1].keys():# each word in the Liwc dictionary- check to see if it is in in the frequency distribution\n",
      "        if i[-1]=='*': #If it is a stemmed version, indicated by the astericks\n",
      "            candidates=[q for q in words if q.startswith(i[0:-1])]\n",
      "            if len(candidates) > 0: #If at least one of these stems is present\n",
      "                tot_stems=0 # initialize a variable that will represent the total number of occurences with a given stem\n",
      "                for k in candidates:\n",
      "                    tot_stems+=freq[k]# Add the frequence for each variation\n",
      "                to_tally=cat[1][i]\n",
      "                for categ in to_tally:\n",
      "                    score[categ]=tot_stems\n",
      "\n",
      "        else:\n",
      "            if i in words:\n",
      "                to_tally=cat[1][i]\n",
      "                for categ in to_tally:\n",
      "                    score[categ]=freq[i]\n",
      "\n",
      "\n",
      "\n",
      "    for i in score.keys():\n",
      "        if i in cat[0].keys():\n",
      "            name=cat[0][i]\n",
      "            score2[name]=int(score[i])\n",
      "\n",
      "    all_words=len(sent_statuses)\n",
      "\n",
      "    survey.tw_liwc[survey.Twitid==j]=[score2]\n",
      "    survey.tw_tot_words[survey.Twitid==j]=all_words\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys, os\n",
      "import pandas as pd\n",
      "import cPickle as pickle\n",
      "import re\n",
      "import numpy as np\n",
      "import nltk\n",
      " \n",
      "\n",
      "os.chdir('C:/Users/Asaf/Documents/GitHub/TwitterIdentity/src/py')\n",
      "\n",
      "\n",
      "\n",
      "survey=pd.read_pickle('DataFrames/SurveySentiment')\n",
      "\n",
      "\n",
      "import string \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#This function creates two dictionaries one for categories, like pronounds posneg emoions and an associated id number, and the other dictionary\n",
      "#is, returned in the function as \"dic\", are the words Liwc uses and the category ids associated with them\n",
      "def read_liwc(filename):\n",
      "    liwc_data = open(filename, 'r')\n",
      "    \n",
      "    mode = 0\n",
      "    cat = {}\n",
      "    dic = {}\n",
      "\n",
      "    for line in liwc_data:\n",
      "        line = line.strip('\\r\\n')\n",
      "        if line == '%':\n",
      "            mode += 1\n",
      "            continue\n",
      "        elif mode == 1: # cat\n",
      "            chunks = line.split('\\t')\n",
      "            cat[chunks[0]] = chunks[1]\n",
      "        elif mode == 2: # dic\n",
      "            chunks = line.split('\\t')\n",
      "            word = chunks.pop(0)\n",
      "            dic[word] = chunks\n",
      "    return (cat, dic)\n",
      "\n",
      "\n",
      "survey['fb_liwc']=None #Initialize column for Liwc scores.\n",
      "survey['fb_tot_words']=None \n",
      "\n",
      "#Stuff for crawling each file individually\n",
      "\n",
      "all_files=[]\n",
      "\n",
      "\n",
      "path= 'B:/Participants/'\n",
      "\n",
      "for i in survey['FBid']:\n",
      "    path_full=path+i+'/statuses'\n",
      "    sent_statuses=''\n",
      "    \n",
      "    if os.path.exists(path_full):\n",
      "        all_files.append(path_full)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "showing info http://nltk.github.com/nltk_data/\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import codecs\n",
      "cat=read_liwc('liwc_file.txt')\n",
      "for file_name in all_files[0:11]:\n",
      "\n",
      "\n",
      "    z=pickle.load(open(file_name))\n",
      "    # I took out the bit about opening as a binary, it doesn't work some files, and it screws up the the indexing\n",
      "    #basically it adds an /r to the end of keys, which makes it more cumbersome. \n",
      "    #Now we want to pull out the all out each status for each person, and then look at the stentiment for that status.\n",
      "    #We also want to store the sentiments for each person in another pickle file. This way later we can look at distributions and stuff\n",
      "    #for each person initilize a new list that will be the sentiment for all their status\n",
      "    sent_statuses=''\n",
      "    temp=re.findall(r'\\d+',file_name)\n",
      "    sent_path='B:/Participants/'+temp[0]+'/status_sentiment'\n",
      "\n",
      "    for status_set in z:\n",
      "        for status_list in status_set['data']:\n",
      "            if 'message' in status_list.keys():\n",
      "                    msg=status_list['message'].encode('utf-8')\n",
      "                    sent_statuses += ' ' + msg\n",
      "    sent_statuses=nltk.word_tokenize(sent_statuses.lower().decode('utf-8')) \n",
      "    freq=nltk.FreqDist(sent_statuses) #Just use nltk to do the work of creating the frequency distribution\n",
      "    words=freq.keys() #Gives us each word in someone's set of statuses\n",
      "    score={}\n",
      "    score2={}\n",
      "    for entry in cat[0].keys(): #Go through each category, like \"work\", \"posneg emotions\", etc\n",
      "        score[entry]=0 #In the new dictionary, \"score\" just initialize a key for that category, with a value of 0\n",
      "\n",
      "#for now I can add up all all the words used in Tweets- but it won't be entirely accurate\n",
      "#because of the format and the way things were tokenized. I can do relative scores for distributions. \n",
      "\n",
      "    for i in cat[1].keys():# each word in the Liwc dictionary- check to see if it is in in the frequency distribution\n",
      "        if i[-1]=='*': #If it is a stemmed version, indicated by the astericks\n",
      "            candidates=[q for q in words if q.startswith(i[0:-1])]\n",
      "            if len(candidates) > 0: #If at least one of these stems is present\n",
      "                tot_stems=0 # initialize a variable that will represent the total number of occurences with a given stem\n",
      "                for k in candidates:\n",
      "                    tot_stems+=freq[k]# Add the frequence for each variation\n",
      "                    to_tally=cat[1][i]\n",
      "                for categ in to_tally:\n",
      "                    score[categ]=tot_stems\n",
      "\n",
      "        else:\n",
      "            if i in words:\n",
      "                to_tally=cat[1][i]\n",
      "                for categ in to_tally:\n",
      "                    score[categ]=freq[i]\n",
      "\n",
      "\n",
      "\n",
      "    for i in score.keys():\n",
      "        if i in cat[0].keys():\n",
      "            name=cat[0][i]\n",
      "            score2[name]=int(score[i])\n",
      "\n",
      "    all_words=len(sent_statuses)\n",
      "     #Note I seem to have to do embed the dictionary in a list, otherwise it won't store to the DataFrame\n",
      "    survey.fb_liwc[survey.FBid==temp[0]]=[score2] \n",
      "    survey.fb_tot_words[survey.FBid==temp[0]]=all_words\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "temp\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "['100000967142850']"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y=survey[survey.FBid=='100000967142850']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z=y.fb_liwc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(z)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "pandas.core.series.Series"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "1    {u'relig': 1, u'money': 1, u'pronoun': 1, u'fu...\n",
        "Name: fb_liwc, dtype: object"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y.fb_liwc.keys()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "Int64Index([1], dtype='int64')"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k=survey.fb_liwc[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}